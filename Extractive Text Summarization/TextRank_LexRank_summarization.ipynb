{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT - 550 Information Retrieval Assignment - 10\n",
    "### Student ID - 202011032\n",
    "### Text Summarization using TextRank and LexRank\n",
    "- Library - sumy\n",
    "- Dataset - BBC Business News<br>\n",
    "- Evaluation metrics - ROUGE-1 Score, ROUGE-2 Score<br>\n",
    "- Sentences Count - 10, 15, 20, 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and initializing paths and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.evaluation.rouge import rouge_1, rouge_2\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "# Initialize paths and configurations\n",
    "dataset_path = os.path.join(\"BBC Business News\",\"News Articles\",\"business\")\n",
    "summaries_path = os.path.join(\"BBC Business News\",\"Summaries\",\"business\")\n",
    "\n",
    "language = \"english\"\n",
    "sentence_counts = (10, 15, 20, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for experimenting with textrank and lexrank summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing LexRank and TextRank summarization techniques for the dataset\n",
    "def auto_textsummarize_dataset(dataset_path, language, sentence_count, text_or_lex='text'):\n",
    "    # Initialize dictionary which will store the generated summaries for all the documents in the path\n",
    "    summary_sentences_list = []\n",
    "    \n",
    "    # Initialize summarizer as per the argument, stemmer and stopwords as per the language\n",
    "    stemmer = Stemmer(language)\n",
    "    summarizer = TextRankSummarizer(stemmer) if text_or_lex == 'text' else LexRankSummarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(language)\n",
    "    \n",
    "    doc_paths = [os.path.join(dp, fname) for dp, df, filepaths in os.walk(dataset_path) for fname in filepaths]\n",
    "    \n",
    "    for doc_path in doc_paths:\n",
    "        # Parse and tokenize the text in the file\n",
    "        doc_parser = PlaintextParser.from_file(doc_path, Tokenizer(language))\n",
    "        \n",
    "        # Create a list of sentences from the generated summary using the given sentence count\n",
    "        # and store it in dictionary\n",
    "        summary_sentences_list.append([sentence for sentence in summarizer(doc_parser.document, sentence_count)])\n",
    "\n",
    "    return summary_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function to evaluate the summaries\n",
    "def evaluate_rouge_1_2(dataset_path, ref_summaries_path, language, sentence_count, **kwargs):\n",
    "    summ_sent_list = auto_textsummarize_dataset(dataset_path, language, sentence_count, **kwargs)\n",
    "    \n",
    "    avg_rouge_1_score = []\n",
    "    avg_rouge_2_score = []\n",
    "    \n",
    "    ref_summ_paths = [os.path.join(dp, fname) \n",
    "                      for dp, df, filepaths in os.walk(ref_summaries_path) \n",
    "                      for fname in filepaths]\n",
    "    \n",
    "    for ref_summ_path, summ_sent in zip(ref_summ_paths, summ_sent_list):\n",
    "        ref_summ_parser = PlaintextParser.from_file(ref_summ_path, Tokenizer(language))\n",
    "        \n",
    "        avg_rouge_1_score.append(rouge_1(summ_sent, ref_summ_parser.document.sentences))\n",
    "        avg_rouge_2_score.append(rouge_2(summ_sent, ref_summ_parser.document.sentences))\n",
    "    \n",
    "    avg_rouge_1_score = sum(avg_rouge_1_score) / len(avg_rouge_1_score)\n",
    "    avg_rouge_2_score = sum(avg_rouge_2_score) / len(avg_rouge_2_score)\n",
    "    \n",
    "    print(f\"For sentence count {sentence_count}:\\n\")\n",
    "    print(f\"Average Rouge 1 Score: {avg_rouge_1_score}\")\n",
    "    print(f\"Average Rouge 2 Score: {avg_rouge_2_score}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating summaries and getting Rouge scores\n",
    "- Summaries are generated from the dataset using TextRank and LexRank summarizers\n",
    "- Different sentence counts are taken and their average rouge 1 and rouge 2 scores are calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with TextRank Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank Summarizer:\n",
      "\n",
      "For sentence count 10:\n",
      "\n",
      "Average Rouge 1 Score: 0.9362589465827755\n",
      "Average Rouge 2 Score: 0.8757167024716055\n",
      "\n",
      "\n",
      "For sentence count 15:\n",
      "\n",
      "Average Rouge 1 Score: 0.9851731870285183\n",
      "Average Rouge 2 Score: 0.9386496636103454\n",
      "\n",
      "\n",
      "For sentence count 20:\n",
      "\n",
      "Average Rouge 1 Score: 0.9949702892728245\n",
      "Average Rouge 2 Score: 0.9512304456123413\n",
      "\n",
      "\n",
      "For sentence count 25:\n",
      "\n",
      "Average Rouge 1 Score: 0.9984427176241695\n",
      "Average Rouge 2 Score: 0.9559380390789242\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TextRank Summarizer:\\n\")\n",
    "for sentence_count in sentence_counts:\n",
    "    evaluate_rouge_1_2(dataset_path, summaries_path, language, sentence_count, text_or_lex='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with LexRank Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexRank Summarizer:\n",
      "\n",
      "For sentence count 10:\n",
      "\n",
      "Average Rouge 1 Score: 0.8040694784872598\n",
      "Average Rouge 2 Score: 0.6988370935573947\n",
      "\n",
      "\n",
      "For sentence count 15:\n",
      "\n",
      "Average Rouge 1 Score: 0.9335514662925445\n",
      "Average Rouge 2 Score: 0.8672927324190142\n",
      "\n",
      "\n",
      "For sentence count 20:\n",
      "\n",
      "Average Rouge 1 Score: 0.9770305291537181\n",
      "Average Rouge 2 Score: 0.9267516799191188\n",
      "\n",
      "\n",
      "For sentence count 25:\n",
      "\n",
      "Average Rouge 1 Score: 0.9921044660033763\n",
      "Average Rouge 2 Score: 0.9472765991587428\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LexRank Summarizer:\\n\")\n",
    "for sentence_count in sentence_counts:\n",
    "    evaluate_rouge_1_2(dataset_path, summaries_path, language, sentence_count, text_or_lex='lex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
